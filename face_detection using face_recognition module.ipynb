{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "designed-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cooked-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the images\n",
    "img_hrithik1 = face_recognition.load_image_file('imagedetec/Hrithik-Roshan-1.jpg') #bgr #returns an array representation of images\n",
    "img_hrithik1 = cv2.cvtColor(img_hrithik1, cv2.COLOR_BGR2RGB) # BGR TO RGB\n",
    "img_hrithik2 = face_recognition.load_image_file('imagedetec/Ratan_tata.jpg')\n",
    "img_hrithik2 = cv2.cvtColor(img_hrithik2, cv2.COLOR_BGR2RGB)\n",
    "img_ratantata = face_recognition.load_image_file('imagedetec/Ratan_tata.jpg')\n",
    "img_ratantata = cv2.cvtColor(img_ratantata, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "physical-idaho",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((563, 750, 3), (702, 512, 3), (702, 512, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_hrithik1.shape, img_hrithik2.shape, img_ratantata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "geological-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locating face location in train image\n",
    "face_loc = face_recognition.face_locations(img_hrithik1)[0]\n",
    "hrithik_encod = face_recognition.face_encodings(img_hrithik1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "involved-helmet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 527, 409, 260),\n",
       " array([-9.82032418e-02,  1.23916805e-01,  1.01674996e-01, -8.27004910e-02,\n",
       "        -1.20795101e-01,  8.33639130e-03, -4.58095968e-02, -7.79753923e-02,\n",
       "         1.03093952e-01,  1.98765658e-04,  1.34342775e-01, -1.38080642e-02,\n",
       "        -2.72528946e-01,  9.58857387e-02, -2.18442157e-02,  6.77506626e-02,\n",
       "        -1.95460230e-01, -1.07855462e-01, -9.78895202e-02, -4.80119623e-02,\n",
       "        -1.48690585e-02,  1.01422735e-01, -1.38067054e-02,  5.22496700e-02,\n",
       "        -6.71409667e-02, -2.95457691e-01, -7.72568956e-02, -1.03088774e-01,\n",
       "         9.85174924e-02, -5.06266281e-02,  1.43722845e-02, -1.65149178e-02,\n",
       "        -2.32358575e-01, -8.34491476e-02,  1.13417497e-02, -4.48134169e-03,\n",
       "        -3.40195708e-02, -6.65993616e-02,  1.91092476e-01,  2.03121938e-02,\n",
       "        -8.79732892e-02,  2.89051477e-02,  1.34244606e-01,  2.94710666e-01,\n",
       "         9.52666923e-02,  1.12161301e-01,  3.34046185e-02,  2.84326635e-03,\n",
       "         8.04169029e-02, -2.46345416e-01,  1.38993889e-01,  1.02930017e-01,\n",
       "         1.54483855e-01,  8.91503990e-02,  9.93635952e-02, -2.70084292e-01,\n",
       "         4.78278771e-02,  1.17552124e-01, -2.13805974e-01,  2.94024050e-01,\n",
       "         6.88064471e-02, -3.38088907e-02,  1.65700875e-02,  1.23079186e-02,\n",
       "         2.63670444e-01,  7.27893189e-02, -6.41244352e-02, -1.62756085e-01,\n",
       "         1.26909479e-01, -1.22367270e-01, -7.21810162e-02,  4.42330465e-02,\n",
       "        -1.14012264e-01, -9.62158665e-02, -2.77157575e-01,  6.49347603e-02,\n",
       "         3.43855858e-01,  9.38657820e-02, -1.93162382e-01,  4.89227567e-03,\n",
       "        -2.24037562e-02, -8.56156051e-02,  4.00231406e-02,  6.63854405e-02,\n",
       "        -9.86931622e-02, -5.87447435e-02, -1.12874530e-01,  5.02719097e-02,\n",
       "         2.18624458e-01,  1.98986009e-02, -3.14290859e-02,  2.71657914e-01,\n",
       "         2.68553998e-02, -5.26629016e-02, -3.87641508e-03,  6.20263629e-02,\n",
       "        -1.55818090e-01, -3.57389599e-02, -1.15227103e-01, -1.20760156e-02,\n",
       "         8.60765725e-02, -5.62169030e-02, -2.23817676e-02,  1.66404456e-01,\n",
       "        -1.88626096e-01,  7.95125291e-02, -3.38780917e-02, -4.86748889e-02,\n",
       "        -3.51942740e-02,  4.89399843e-02, -1.45592511e-01, -8.13528523e-02,\n",
       "         1.90631717e-01, -2.26500079e-01,  2.39544034e-01,  1.87221602e-01,\n",
       "         1.76165476e-02,  5.80033883e-02,  4.23758291e-03,  1.59962960e-02,\n",
       "         9.17780176e-02,  6.80723786e-02, -1.39482707e-01, -1.23109818e-01,\n",
       "         5.66330701e-02, -6.67457581e-02,  6.79437909e-03,  1.23839997e-01]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_loc, hrithik_encod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "virgin-carroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[155, 138, 129],\n",
       "        [155, 138, 129],\n",
       "        [155, 138, 129],\n",
       "        ...,\n",
       "        [163, 158, 157],\n",
       "        [163, 158, 157],\n",
       "        [163, 158, 157]],\n",
       "\n",
       "       [[154, 137, 128],\n",
       "        [154, 137, 128],\n",
       "        [154, 137, 128],\n",
       "        ...,\n",
       "        [163, 158, 157],\n",
       "        [163, 158, 157],\n",
       "        [163, 158, 157]],\n",
       "\n",
       "       [[153, 136, 127],\n",
       "        [153, 136, 127],\n",
       "        [154, 137, 128],\n",
       "        ...,\n",
       "        [163, 158, 157],\n",
       "        [163, 158, 157],\n",
       "        [163, 158, 157]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 97,  71,  64],\n",
       "        [ 94,  68,  61],\n",
       "        [ 84,  61,  53],\n",
       "        ...,\n",
       "        [194, 176, 165],\n",
       "        [194, 176, 165],\n",
       "        [194, 176, 165]],\n",
       "\n",
       "       [[ 81,  57,  51],\n",
       "        [ 81,  57,  51],\n",
       "        [ 76,  54,  48],\n",
       "        ...,\n",
       "        [194, 176, 165],\n",
       "        [194, 176, 165],\n",
       "        [194, 176, 165]],\n",
       "\n",
       "       [[ 70,  46,  40],\n",
       "        [ 73,  49,  43],\n",
       "        [ 72,  50,  44],\n",
       "        ...,\n",
       "        [194, 176, 165],\n",
       "        [194, 176, 165],\n",
       "        [194, 176, 165]]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we have now the face location coordinates of train image lets draw a rectangle around them using opencv\n",
    "cv2.rectangle(img_hrithik1,(face_loc[3],face_loc[0]),(face_loc[1],face_loc[2]),(0,255,0),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "monetary-blowing",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[101, 134, 143],\n",
       "        [102, 138, 144],\n",
       "        [104, 136, 147],\n",
       "        ...,\n",
       "        [113, 143, 154],\n",
       "        [115, 147, 158],\n",
       "        [114, 148, 161]],\n",
       "\n",
       "       [[ 99, 132, 141],\n",
       "        [ 96, 129, 138],\n",
       "        [ 98, 133, 143],\n",
       "        ...,\n",
       "        [113, 142, 156],\n",
       "        [117, 149, 162],\n",
       "        [115, 149, 162]],\n",
       "\n",
       "       [[ 99, 132, 141],\n",
       "        [ 96, 133, 137],\n",
       "        [ 99, 131, 137],\n",
       "        ...,\n",
       "        [114, 145, 160],\n",
       "        [121, 152, 167],\n",
       "        [117, 151, 164]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 91, 117, 104],\n",
       "        [ 88, 114, 101],\n",
       "        [ 90, 110, 105],\n",
       "        ...,\n",
       "        [ 16,   8,   9],\n",
       "        [ 16,   8,   8],\n",
       "        [ 18,  11,   8]],\n",
       "\n",
       "       [[ 86, 112, 112],\n",
       "        [ 83, 115, 110],\n",
       "        [ 88, 116, 117],\n",
       "        ...,\n",
       "        [  9,  10,   6],\n",
       "        [ 10,  11,   9],\n",
       "        [  9,  10,   8]],\n",
       "\n",
       "       [[ 83, 111, 111],\n",
       "        [ 82, 113, 112],\n",
       "        [ 89, 118, 123],\n",
       "        ...,\n",
       "        [ 11,   9,   8],\n",
       "        [ 11,   9,   8],\n",
       "        [ 14,  10,   9]]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# locating face location, encoding the test image then putting the rectangle arounf the face location in test image\n",
    "test_loc = face_recognition.face_locations(img_hrithik2)[0]\n",
    "test_encod = face_recognition.face_encodings(img_hrithik2)[0]\n",
    "cv2.rectangle(img_hrithik2,(test_loc[3],test_loc[0]),(test_loc[1],test_loc[2]),(0,255,0),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-regard",
   "metadata": {},
   "source": [
    "# Step 3 - Comparing the faces using their encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "female-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = face_recognition.compare_faces([hrithik_encod], test_encod)\n",
    "\n",
    "# if we want to check the face distance\n",
    "facedis = face_recognition.face_distance([hrithik_encod], test_encod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "diagnostic-asbestos",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([False], array([0.80006904]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, facedis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "returning-switch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[101, 134, 143],\n",
       "        [102, 138, 144],\n",
       "        [104, 136, 147],\n",
       "        ...,\n",
       "        [113, 143, 154],\n",
       "        [115, 147, 158],\n",
       "        [114, 148, 161]],\n",
       "\n",
       "       [[ 99, 132, 141],\n",
       "        [ 96, 129, 138],\n",
       "        [ 98, 133, 143],\n",
       "        ...,\n",
       "        [113, 142, 156],\n",
       "        [117, 149, 162],\n",
       "        [115, 149, 162]],\n",
       "\n",
       "       [[ 99, 132, 141],\n",
       "        [ 96, 133, 137],\n",
       "        [ 99, 131, 137],\n",
       "        ...,\n",
       "        [114, 145, 160],\n",
       "        [121, 152, 167],\n",
       "        [117, 151, 164]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 91, 117, 104],\n",
       "        [ 88, 114, 101],\n",
       "        [ 90, 110, 105],\n",
       "        ...,\n",
       "        [ 16,   8,   9],\n",
       "        [ 16,   8,   8],\n",
       "        [ 18,  11,   8]],\n",
       "\n",
       "       [[ 86, 112, 112],\n",
       "        [ 83, 115, 110],\n",
       "        [ 88, 116, 117],\n",
       "        ...,\n",
       "        [  9,  10,   6],\n",
       "        [ 10,  11,   9],\n",
       "        [  9,  10,   8]],\n",
       "\n",
       "       [[ 83, 111, 111],\n",
       "        [ 82, 113, 112],\n",
       "        [ 89, 118, 123],\n",
       "        ...,\n",
       "        [ 11,   9,   8],\n",
       "        [ 11,   9,   8],\n",
       "        [ 14,  10,   9]]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting the results on the required images using opencv\n",
    "cv2.putText(img_hrithik2, str(results)+'  'str(round(facedis[0],2)), (100,100), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "technical-thousand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing the images using opencv\n",
    "cv2.imshow('1',img_hrithik1)\n",
    "cv2.imshow('2',img_hrithik2)\n",
    "cv2.imshow('3',img_ratantata)\n",
    "\n",
    "cv2.waitKey(0) #always remember to include it when using cv2.imshow to view the iamges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-elder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-original",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-syria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-character",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
